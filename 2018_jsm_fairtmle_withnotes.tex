\documentclass[12pt,t,handout]{beamer}
\usepackage{graphicx}
\setbeameroption{show notes}
\setbeamertemplate{note page}[plain]
\usepackage{listings}
\usepackage{datetime}
\usepackage{url}

% specifications for presenter mode
%\beamerdefaultoverlayspecification{<+->}
%\setbeamercovered{transparent}

% math shorthand
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{bbm}
\usepackage{bm}
\usepackage{mathtools}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\lik}{\mathcal{L}}
\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{D\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\openr}{\hbox{${\rm I\kern-.2em R}$}}

% Bibliography
\usepackage{natbib}
\bibpunct{(}{)}{,}{a}{}{;}
\usepackage{bibentry}
\nobibliography*

\def\notescolors{1}
\input{header.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end of header
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% title info
\title{\small Fair Inference Through Semiparametric-Efficient Estimation Over
  Constraint-Specific Paths}
\subtitle{\scriptsize for \textit{New Developments in Nonparametric and
    Semiparametric Statistics},\\
    Joint Statistical Meetings; Vancouver, BC, Canada; 02 Aug.~2018\\[-10pt]}
\author{\href{https://nimahejazi.org}{Nima S.~Hejazi}
       \\[-10pt]
       }
\institute{Group in Biostatistics, and\\
           Center for Computational Biology, \\
           University of California, Berkeley \\
           \href{https://statistics.berkeley.edu/~nhejazi}
             {\tt \scriptsize \color{foreground}
               stat.berkeley.edu/\textasciitilde{}nhejazi
             }
           \\[4pt]
           \includegraphics[height=20mm]{Figs/seal-berkeley.png}
           \\[-12pt]
          }
\date{
  \href{https://nimahejazi.org}
      {\tt \scriptsize \color{foreground} nimahejazi.org}
  \\[-4pt]
  \href{https://twitter.com/nshejazi}
      {\tt \scriptsize \color{foreground} Twitter: nshejazi}
  \\[-4pt]
  \href{https://github.com/nhejazi}
      {\tt \scriptsize \color{foreground} GitHub: nhejazi}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

% title slide
{
\setbeamertemplate{footline}{} % no page number here
\frame{
  \titlepage

  \vspace{-1em}

  \centerline{\href{http://bit.ly/jsm_fairtmle_2018}{\tt \scriptsize
      \underline{slides}: bit.ly/jsm\_fairtmle\_2018}}
  \vspace{-1.5em}
  \vfill \hfill \includegraphics[height=6mm]{Figs/cc-zero.png} \vspace*{-0.5cm}

  \note{This slide deck is for a short presentation on new work in Targeted
    Learning, discussing both the construction of constrained ensemble machine
    learning for the construction of ``fair'' estimates and a novel algorithm
    for computing TML estimators with respect to some contraint functional.

    Slides: {\tt bit.ly/jsm\_fairtmle\_2018}
}
}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Preview: Summary}
\only<1>{\addtocounter{framenumber}{-1}}

\begin{center}
\begin{itemize}
  \itemsep12pt
  \item Recent work suggests that the widespread use of machine learning
    algorithms has had negative social and policy consequences.
  \item The construction of ``fair'' machine learning algorithms is an active
    area of diverse research.
  \item We propose a general algorithm for constructing ``fair'' optimal
    ensemble machine learning estimators via cross-validation.
  \item We extend Targeted Learning provides a framework for estimating complex
    parameters in nonparametric (infinite-dimensional) statistical models.
  \item Constraints may be imposed as functionals defined over the target
    parameter of interest.
  \item Estimating constrained parameters may be seen as iteratively minimizing
    a loss function along a \textit{constrained} path in the parameter space
    $\Psi$.
\end{itemize}
\end{center}

\note{We'll go over this summary again at the end of the talk. Hopefully, it
  will all make more sense then.
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile,c]{Fairness is machine learning?}

\begin{center}
\begin{minipage}[c]{11.25cm}
\begin{semiverbatim}
\lstset{basicstyle=\normalsize}
\begin{lstlisting}[linewidth=11.25cm]
  Another potential result: a more diverse
  workplace. The software relies on data
  to surface candidates from a wide variety
  of places...free of human biases. But
  software is not free of human influence.
  Algorithms are written and maintained by
  people...As a result...algorithms can
  reinforce human prejudices.

  -Miller (2015)
\end{lstlisting}
\end{semiverbatim}
\end{minipage}
\end{center}

\note{Obviously, it's important to explain the motivating example here.}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Addressing bias in a technical manner}

\begin{center}
\begin{itemize}
  \itemsep12pt
  \item The careless use of machine learning may induce \textit{unjustified}
    bias.
  \item Problematic discrimination by ML approaches leads to solutions with
    \textit{practical irrelevance}.
  \item Ill-considered discrimination by ML approaches leads to solutions that
    are \textit{morally problematic}.
  \item Two docrtines of discrimination:
    \begin{enumerate}
      \item Disparate treatment: formal or intentional
      \item Disparate impact: unjustified or avoidable
    \end{enumerate}
\end{itemize}
\end{center}

\note{
  Considering and treating bias using a technical approach is an important way
  of dealing with the potential negative consequences of machine learning.
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Background, data, notation}

\begin{center}
\begin{itemize}
  \itemsep12pt
  \item An observational unit: $O = (W, X, Y)$, where $W$ is baseline
    covariates, $X$ a sensitive characteristic, $Y$ an outcome of interest.
  \item Consider $n$ i.i.d.~copies $O_1, \ldots, O_n$ of $O \sim P_0 \in
    \mathcal{M}$.
  \item Here, $\mathcal{M}$ is an infinite-dimensional statistical model (i.e.,
    indexed by an infinite-dimensional vector).
  \item We discuss the estimation of a target parameter $\psi : \mathcal{M}
    \rightarrow \mathbb{R}$, where
    $$\Psi(P_0) = \argmin_{\psi \in \Psi} \E_{P_0}L(\psi)$$
\end{itemize}
\end{center}

\note{
  We just need to see this to get a feel for what's going to be happening with
  the derivation of constraint-specific paths.
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Wait, where did the fairness go?}

\begin{center}
\begin{itemize}
  \itemsep12pt
  \item Goal: estimate $\Psi(P_0) = \E_{P_0}(Y \mid X, W)$.
  \item Let $Y \in \{0, 1\}$ and use neg.~log-likelihood loss:
    $$L(\psi) = -(Y \log(P(Y \mid X, W)) + (1 - Y) \log(1 - P(Y \mid X, W)))$$
  \item Fairness criterion --- \textit{equalized odds}:
    $$\begin{aligned}
      \Theta_{\psi}(P_0) = \sum_y \{&\E_{P_0}(L(\psi)(O) \mid X = 1, Y = y) \\ -
      &\E_{P_0}(L(\psi)(O) \mid X = 0, Y = y)\}^2
    \end{aligned}$$
  \item Let $\Theta_{\psi}(P): \mathcal{M} \rightarrow \openr$ be a pathwise
    differentiable \textit{functional} for each $\psi \in \Psi$.
\end{itemize}
\end{center}

\note{
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Just a few fairness criteria}

\begin{center}
\begin{itemize}
  \itemsep8pt
  \item Let $C: (X, W) \to Y \in \{0, 1\}$ be a classifier; $X \in \{a, b\}$.
  \item Demographic parity: $P_{(X = a)}(C = 1) = P_{(X = b)}(C = 0)$
  \item Accuracy parity: $P_{(X = a)}(C = Y) = P_{(X = b)}(C = Y)$
  \item True positive parity: $P_{(X = a)}(C = 1 \mid Y = 1) = P_{(X = b)}(C =
    1 \mid Y = 1)$
  \item False positive parity: $P_{(X = a)}(C = 1 \mid Y = 0) = P_{(X = b)}(C =
    1 \mid Y = 0)$
  \item Positive predictive value parity: $P_{(X = a)}(Y = 1 \mid C = 1) =
    P_{(X = b)}(Y = 1 \mid C = 1)$
  \item Negative predictive value parity: $P_{(X = a)}(Y = 1 \mid C = 0) =
    P_{(X = b)}(Y = 1 \mid C = 0)$
\end{itemize}
\end{center}

\note{
  It's a jungle out there
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Constrained functional parameters}

\begin{center}
\begin{itemize}
  \itemsep12pt
  \item $$\Psi(P) = \argmin_{\psi \in \Psi, \Theta_{\psi}(P) = 0} \E_PL(\psi)$$
  \item We wish to estimate $\Psi^*(P)$, the projection of $\Psi(P)$ onto the
    subspace $\Psi^*(P) = \{\psi \in \Psi: \Theta_{\psi}(P) = 0\}$
    w.r.t.~loss-based dissimilarity: $d_{P}(\psi, \Psi(P)) = \E_PL(\psi) -
    \E_PL(\Psi(P))$.
  \item $$ \Psi^*(P) = \argmin_{\psi \in \Psi, \Theta_{\psi}(P) = 0} d_{P}(\psi,
    \Psi(P))$$
\end{itemize}

\begin{equation}\label{lagrangedef}
  (\Psi^*, \lambda) = (\Psi^*(P), \Lambda(P)) \equiv \argmin_{\psi \in \Psi,
    \lambda} \E_PL(\psi) + \lambda \Theta_{\psi}(P),
\end{equation}

\begin{itemize}
  \item \textit{Lemma}: If $\widetilde{\Psi}(P) = (\Psi^*(P), \Lambda(P))$ is
    the minimizer of the Lagrange multiplier penalized loss (\ref{lagrangedef}),
    then it readily follows that $\Psi^*(P) = \argmin_{\psi \in {\bf \Psi},
      \Theta_{\psi}(P) = 0} \E_{P}L(\psi)$.
\end{itemize}

\end{center}

\note{
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Learning with constrained parameters}

\begin{center}
\begin{itemize}
  \itemsep12pt
  \item Consider the risk function for $\widetilde{\psi} = (\psi^*, \lambda)$
    --- i.e., $R(\tilde{\psi} \mid P) \equiv PL(\psi^*) + \lambda \Theta(\psi^*
    \mid P)$.
  \item $$\E(\Psi^*(P), \Lambda(P)) = \argmin_{(\psi, \lambda) \in \Psi
      \times \openr_{\geq 0}}R(\psi, \lambda \mid P)$$
  \item For a given estimator $\widetilde{\psi}(P_n) = (\hat{\Psi}^*(P_n),
    \hat{\lambda}(P_n))$ of $\widetilde{\Psi}(P_0)$, and a cross-validation
    scheme defined by a distribution of $B_n \in \{0, 1\}^n$ that splits the
    sample.
  \item We can define a conditional risk as follows:
    $$R_0(\widetilde{\psi}, P_n) = \E_{B_n} P_0L(\hat{\Psi}^*(P_{n, B_n}^0)) +
      \hat{\Lambda}(P_n) \E_{B_n} \Theta(\hat{\Psi}^*(P_{n, B_n}^0) \mid P_0)$$
\end{itemize}
\end{center}

\note{
  \begin{itemize}
    \item Here $P_{n, B_n}^0$ denotes the empirical distribution of the training
      sample.
  \end{itemize}
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Learning with constrained parameters}

\begin{center}
\begin{equation}
\begin{split}
  R_{n, CV}(\tilde{\psi}, P_n) = E_{B_n}P_{n, B_n}^1L(\hat{\Psi}^*(P_{n,
    B_n}^0)) \\ + \hat{\Lambda}(P_n) E_{B_n} \Theta(\hat{\Psi}^*(P_{n, B_n}^0)
    \mid P_{n, B_n}^*)
\end{split}
\end{equation}
\begin{itemize}
  \itemsep12pt
  \item Given a set of candidate estimators $\widetilde{\psi}_j(P_n) =
    (\hat{\Psi}_j^*(P_n), \hat{\Lambda}_j(P_n))$, $j = 1, \ldots, J$, the
    cross-validation selector is given by: $J_n = \argmin_j R_{n,
      CV}(\widetilde{\psi}_j, P_n)$
  \item A Super Learner may be defined over a set of candidate nonparametric
    regression functions (i.e., machine learning algorithms).
  \item Here, we may define a Super Learner of $\widetilde{\Psi}$ by
    $\widetilde{\psi}_n \equiv \widetilde{\psi}_{J_n}(P_n) =
    (\hat{\Psi}_{J_n}(P_n), \hat{\lambda}_{J_n}(P_n))$
\end{itemize}
\end{center}

\note{
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Mappings with constrained learners}

\begin{center}
A straightforward approach to generating estimators of the constrained parameter
would be to simply generate a mapping according to the following simple process:

\vspace*{1em}

\begin{enumerate}
  \itemsep12pt
  \item Generate an unconstrained Super Learner $\psi_n$ of the unconstrained
    parameter $\psi_0$,
  \item Map an estimator $\Theta_{\psi_n, n}$ of the constraint
    $\Theta_{\psi_n}(P_0)$ into the path $\psi_{n, \lambda}$ mentioned above.
    The corresponding solution $\psi_n^* = \psi_{n, \lambda_n}$ of
    $\Theta_{\psi_{n, \lambda_n}, n}= 0$ generates an estimator of the
    constrained parameter.
\end{enumerate}
\end{center}

\note{
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Constraint-specific paths}

\begin{center}

Let $P_0 \in \mathcal{M}$ be the true distribution of the data $O = (W, X, Y)$,
contained in an infinite-dimensional statistical model $\mathcal{M}$.

\vspace{1em}

Consider the solution $\psi_{0, \lambda} = \argmax_{\psi \in \Psi}
\E_{P_0}L(\psi) + \lambda \Theta_0(\psi)$, noting that $\{\psi_{0, \lambda} :
\lambda\}$ represents a path in the parameter space $\Psi$ through $\psi_0$ at
$\lambda = 0$, which we refer to as the \textit{constraint-specific path}.

\vspace{1em}

We then leverage this construction to map an initial estimator of the
unconstrained parameter $\psi_0$ into its corresponding constrained version
$\psi_0^*$.

\end{center}

\note{
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Constraint-specific paths}

\begin{center}
\underline{The fine print}:

\scriptsize{
\begin{theorem}
Let $\{\psi_{\delta, H}: \delta \}$ be a one-dimensional path through $\psi$ at
$\delta = 0$ with direction $H \in {\bf H}(\psi)$, where $H$ varies over a set
of directions $\mathcal{H}$. Let $H(\psi)$ be a Hilbert space with inner product
$\langle f, g \rangle$.

\vspace{1em}

Let $T(\psi) \subset H(\psi)$ be the closure of the linear span of ${\cal H}$.
Let $D_{0, L}(\psi) \in T(\psi)$ be the canonical gradient of the pathwise
derivative $\left . \frac{d}{d \delta} \E_{P_0}L(\psi_{\delta, H}) \right
|_{\delta = 0} = \langle D_{0, L}(\psi), H \rangle$. Let $D_{0, \Theta}(\psi)
\in T(\psi)$ be the canonical gradient of the pathwise derivative $\left .
 \frac{d}{d \delta}\Theta_0(\psi_{\delta, H}) \right |_{\delta = 0} =
 \langle D_{0, \Theta}(\psi), H \rangle$. Let
$\psi_{0, \lambda} = \argmin_{\psi \in {\bf \Psi}} \E_{P_0}L(\psi) + \lambda
\Theta_0(\psi)$. Suppose that $\psi_{0, \lambda}$ solves its score equations
$\left . \frac{d}{d \delta} \E_{P_0}L(\psi_{0, \lambda, \delta, H}) \right
|_{\delta = 0}$ for all paths. Then, we have
\begin{equation}\label{lfmpathconditiona}
0 = D_L(\psi_{0, \lambda}) + \lambda D_{0, \Theta}(\psi_{0, \lambda}).
\end{equation}

$\dots$\\

This now uniquely defines the solution $\psi_{0, \lambda}$ and an algorithm for
computing it, given above. Finally, letting $\psi_0^* = \argmin_{\psi \in {\bf
\Psi}, \Theta_0(\psi) = 0} \E_{P_0}L(\psi)$, we have $\psi_0^* = \psi_{0,
 \lambda_0}$, where $\lambda_0$ is such that $\Theta_0(\psi_{0, \lambda}) = 0$.
\end{theorem}
}

\end{center}

\note{
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Future work}

\begin{center}
\begin{itemize}
  \itemsep12pt
  \item Further generalization of constraint-specific paths: the solution path
    $\{\psi_{0, \lambda}: \lambda\}$ in the parameter space $\Psi$ through
    $\psi_0$ at $\lambda = 0$.
  \item Further develop relation between constraint-specific paths and universal
    least favorable submodels (Efron's least favorable families).
  \item Integration of the approach of constraint-specific paths with classical
    classical targeted maximum likelihood estimation --- in particular, what, if
    any, are the implications for inference?
  \item Try our approach out with practical constraint-type problems (e.g.,
    fairness via equalized odds, physician knowledge in prediction).
\end{itemize}
\end{center}

\note{
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Review: Summary}

\begin{center}
\begin{itemize}
  \itemsep12pt
  \item Targeted Learning provides a framework for estimating complex parameters
    in nonparametric (infinite-dimensional) statistical models.
  \item Constraints may be imposed as functionals defined over the target
    parameter of interest.
  \item Estimating constrained parameters may be seen as iteratively minimizing
    a loss function along a \textit{constrained} path in the parameter space
    $\Psi$.
  \item Optimal nonparametric estimates of parameters (and constituent parts
    thereof) may be obtained with Super Learning (i.e., stacked regression,
    ensemble models).
\end{itemize}
\end{center}

\note{It's always good to include a summary.}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% don't want dimming with references
%\setbeamercovered{}
%\beamerdefaultoverlayspecification{}

\begin{frame}[c,allowframebreaks]{References}

\bibliographystyle{apalike}
\nocite{*}
\bibliography{references}

%\note{Here's some work we've talked about. Go check these out if interested.}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Acknowledgments}

\vspace{18pt}

\begin{tabular}{@{}l@{\hspace{1.5cm}}l@{}}
Mark J.~van der Laan & \footnotesize \lolit University of California, Berkeley
\end{tabular}

\vspace{10mm}

\underline{Funding source}:\\
National Library of Medicine (of NIH): T32-LM012417--02

\note{
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Thank you.}

\Large
Slides: \href{https://bit.ly/jsm_fairtmle_2018}{bit.ly/jsm\_fairtmle\_2018}
\quad \includegraphics[height=5mm]{Figs/cc-zero.png}

\vspace{3mm}
\href{https://www.stat.berkeley.edu/~nhejazi}{\tt
  stat.berkeley.edu/\textasciitilde{}nhejazi}

\vspace{3mm}
\href{http://nimahejazi.org}{\tt nimahejazi.org}

\vspace{3mm}
\href{https://twitter.com/nshejazi}{\tt Twitter: nshejazi}

\vspace{3mm}
\href{https://github.com/nhejazi}{\tt GitHub: nhejazi}

\note{Here's where you can find me, as well as the slides for this talk.}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

